{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Classification avec le jeu de données TITANIC </H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif est de poursuivre le TP précédente où de nombreux prétraitements ont été effecutés en ingénierie des données pour pouvoir faire un modèle de prédiction des survivants ou non.\n",
    "\n",
    "Executer les importations suivantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Sickit learn met régulièrement à jour des versions et indique des futurs warnings. \n",
    "#ces deux lignes permettent de ne pas les afficher.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous n'avez pas terminé le TP précédent vous pouvez récupérer le fichier titanic2.csv qui contient les différentes transformations. Le mettre dans le répertoire Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Dataset/titanic2.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut maintenant définir les variables d'apprentissage et la variable à prédire. Ici Suvived est la variable à prédire. \n",
    "Attention elle est positionné à la première colonne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values\n",
    "X = array[:,1:8] \n",
    "y = array[:,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Découper le jeu de données en jeu de test et jeu d'apprentissage. Prenez 30% du jeu de données pour le test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_size=0.3 #30% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                 train_size=validation_size, \n",
    "                 random_state=seed,test_size=testsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Essai d'un classifieur **  \n",
    "\n",
    "Avec le classifieur GaussianNB effectuer une première prédiction en donnant la valeur de l'accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "result = clf.predict(X_test)\n",
    "\n",
    "print('\\n accuracy :', \n",
    "      accuracy_score(result, y_test),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher la matrice de confusion et le classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_test, result)\n",
    "print ('\\n matrice de confusion \\n',conf)\n",
    "\n",
    "print ('\\n',classification_report(y_test, \n",
    "                                  result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Effectuer une cross validation avec 10 splits (Kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "k_fold = KFold(n_splits=10, \n",
    "               shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer le classifieur GaussianNB et donner les différentes accuracy pour les 10 évaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, X, y, \n",
    "                        cv=k_fold, scoring=scoring)\n",
    "\n",
    "print('Les différentes accuracy pour les 10 évaluations sont : \\n',score,'\\n')\n",
    "print ('Accuracy moyenne : ',score.mean(),\n",
    "       ' standard deviation', score.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Essai de plusieurs classifieurs **\n",
    "\n",
    "Utiliser à présent différents classifieurs : KNeighborsClassifier, DecisionTreeClassifier, GaussianNB, SVC et RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "models.append(('RFO', RandomForestClassifier()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quels sont les résultats des classifications ? Quel est le classifieur le plus efficace ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name,model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X, \n",
    "                                 y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), \n",
    "                           cv_results.std())\n",
    "    print(msg)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de boxplot afficher les résultats des différents classifieurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Comparaison des algorithmes')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classifieur RandomForestClassifier donne de meilleurs résultats. A l'aide de GridSearch, évaluer les différents paramètres :  \n",
    "  grid_param = {'n_estimators': [4, 6, 9],   \n",
    "     'max_features': ['log2', 'sqrt','auto'],   \n",
    "     'criterion': ['entropy', 'gini'],  \n",
    "     'max_depth': [2, 3, 5, 10],   \n",
    "     'min_samples_split': [2, 3, 5],  \n",
    "     'min_samples_leaf': [1,5,8]  \n",
    "      }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_param = {'n_estimators': [4, 6, 9], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,5,8]\n",
    "             }\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=RandomForestClassifier(),  \n",
    "                     param_grid=grid_param,\n",
    "                     scoring='accuracy',\n",
    "                     cv=5,\n",
    "                     n_jobs=-1,\n",
    "                    iid=True,\n",
    "                    return_train_score=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle est la meilleure accuracy et pour quels paramètres ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gd_sr.fit(X_train, y_train)  \n",
    "print ('meilleur score ',\n",
    "       gd_sr.best_score_,'\\n')\n",
    "print ('meilleurs paramètres', \n",
    "       gd_sr.best_params_,'\\n')\n",
    "print ('meilleur estimateur',\n",
    "       gd_sr.best_estimator_,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB obtient un assez bon score mais il n'a pas d'hyperparamètres à rechercher. Il est suivi par DecisisionTreeClassifier et SVC.   \n",
    "\n",
    "Appliquer GridSearchCV pour ces deux classifieurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "grid_param = {  \n",
    "    'max_depth': [1,2,3,4,5,6,7,8,9,10],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=DecisionTreeClassifier(),  \n",
    "                     param_grid=grid_param,\n",
    "                     scoring='accuracy',\n",
    "                     cv=10,\n",
    "                     n_jobs=-1,\n",
    "                    iid=True,\n",
    "                    return_train_score=True)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)  \n",
    "\n",
    "gd_sr.fit(X_train, y_train)  \n",
    "print ('meilleur score ',gd_sr.best_score_,'\\n')\n",
    "print ('meilleurs paramètres', gd_sr.best_params_,'\\n')\n",
    "print ('meilleur estimateur',gd_sr.best_estimator_,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10], \n",
    "    'gamma' : [0.001, 0.01, 0.1, 1], \n",
    "    'kernel': ['linear','rbf']}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=SVC(),  \n",
    "                     param_grid=grid_param,\n",
    "                     scoring='accuracy',\n",
    "                     cv=5,\n",
    "                     n_jobs=1,\n",
    "                     iid=True,\n",
    "                    return_train_score=True)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)  \n",
    "\n",
    "print ('meilleur score ',\n",
    "       gd_sr.best_score_,'\\n')\n",
    "print ('meilleurs paramètres', \n",
    "       gd_sr.best_params_,'\\n')\n",
    "print ('meilleur estimateur',\n",
    "       gd_sr.best_estimator_,'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous constatez que RandomForestClassifier et DecisionTreeClassifier peuvent avoir la même accuracy.  \n",
    "Faire un gridsearch avec les deux classifieurs en prenant les paramètres précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "params = {'RandomForestClassifier' : \n",
    "          [{'n_estimators': [4, 6, 9]}, \n",
    "            {'max_features': ['log2', 'sqrt','auto']}, \n",
    "            {'criterion': ['entropy', 'gini']},\n",
    "            {'max_depth': [2, 3, 5, 10]}, \n",
    "            {'min_samples_split': [2, 3, 5]},\n",
    "            {'min_samples_leaf': [1,5,8]}],\n",
    "           'DecisionTreeClassifier': \n",
    "          [{'max_depth': [1,2,3,4,5,6,7,8,9,10]},\n",
    "    {'criterion': ['gini', 'entropy']},\n",
    "    {'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10]}]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "     def __init__(self,name, score, parameters):\n",
    "         self.name = name\n",
    "         self.score = score\n",
    "         self.parameters = parameters\n",
    "     def __repr__(self):\n",
    "         return repr((self.name, self.score, self.parameters))\n",
    "\n",
    "       \n",
    "results = []\n",
    "for key,value in classifiers.items():\n",
    "    gd_sr = GridSearchCV(estimator=value,  \n",
    "                     param_grid=params[key],\n",
    "                     scoring='accuracy',\n",
    "                     cv=10,\n",
    "                     n_jobs=1,\n",
    "                     iid=True)\n",
    "    gd_sr.fit(X_train, y_train)  \n",
    "    result=Result(key,gd_sr.best_score_,\n",
    "                  gd_sr.best_estimator_)\n",
    "    results.append(result)   \n",
    "    \n",
    "    \n",
    "    \n",
    "results=sorted(results, \n",
    "               key=lambda result: result.score, \n",
    "               reverse=True) \n",
    "\n",
    "print ('Le meilleur resultat : \\n')\n",
    "print ('Classifier : ',results[0].name, \n",
    "       ' score %0.4f' %results[0].score, \n",
    "       ' avec ',results[0].parameters,'\\n')\n",
    "\n",
    "print ('Tous les résultats : \\n')\n",
    "for result in results:\n",
    "    print ('Classifier : ',result.name,\n",
    "           ' score %0.4f' %result.score,\n",
    "           ' avec ',result.parameters,'\\n')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion ** : il faut toujours faire attention aux paramètres passés dans un classifieur. Ils peuvent tout changer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sauvegarde du modèle**\n",
    "\n",
    "Sauvegarder le meilleur modèle appris et recharger le pour le tester avec y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=gd_sr.best_estimator_\n",
    "import pickle\n",
    "filename = 'pkl_Titanicbestmodel.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_loaded = pickle.load(open(filename, 'rb'))\n",
    "print ('Modèle chargé',clf_loaded,'\\n')\n",
    "result = clf_loaded.predict(X_test)\n",
    "\n",
    "print('\\n accuracy:\\n')\n",
    "print (accuracy_score(result, y_test),'\\n')\n",
    "\n",
    "conf = confusion_matrix(y_test, result)\n",
    "print ('\\n matrice de confusion \\n',conf)\n",
    "print ('\\n',classification_report(y_test, result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
